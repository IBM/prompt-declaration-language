{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29514fc-9383-490c-8a73-63c547c21d95",
   "metadata": {},
   "source": [
    "# Prompt Declaration Language\n",
    "\n",
    "Prompt engineering is difficult: minor variations in prompts have large impacts on the output of LLMs and prompts are model-dependent. In recent years <i> prompt programming languages </i> have emerged to bring discipline to prompt engineering. Many of them are embedded in an imperative language such as Python or TypeScript, making it difficult for users to directly interact with prompts and multi-turn LLM interactions.\n",
    "\n",
    "The Prompt Declaration Language (PDL) is a YAML-based declarative approach to prompt programming, where prompts are at the forefront. PDL facilitates model chaining and tool use, abstracting away the plumbing necessary for such compositions, enables type checking of the input and output of models, and is based on LiteLLM to support a variety of model providers. PDL has been used with RAG, CoT, ReAct, and an agent for solving SWE-bench.\n",
    "\n",
    "All examples in this notebook use the new ibm/granite-8b-instruct-preview-4k model. You can use PDL stand-alone or from a Python SDK or, as shown here, in a notebook via a notebook extension. In the cell output, model-generated text is rendered in green font, and tool-generated text is rendered in purple font."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e7ba6-e0f0-4d88-a083-5ff257ed2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'prompt-declaration-language[examples]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a6874-54d9-4167-82ed-ab2f4fdc0a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pdl.pdl_notebook_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a750b-5765-4e2a-9dc1-9c9af2eab940",
   "metadata": {},
   "source": [
    "## Model call\n",
    "\n",
    "In PDL, the user specifies step-by-step the shape of data they want to generate. In the following, the `text` construct indicates a text block containing a prompt and a model call. Implicitly, PDL builds a background conversational context (list of role/content) which is used to make model calls. Each model call uses the context built so far as its input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c62df1-0347-4711-acd7-3892cfd5df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pdl --reset-context\n",
    "text: \n",
    "- \"What is the meaning of life?\\n\"\n",
    "- model: \"replicate/ibm-granite/granite-3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41eb56-e278-4024-9979-7c3410e9ccf5",
   "metadata": {},
   "source": [
    "## Model chaining\n",
    "Model chaining can be done by simply adding to the list of models to call declaratively. Since this cell has the `%%pdl` cell magic without `--reset-context`, it executes in the context created by the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c323b-ad1a-4434-8732-bc19c5c47883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pdl\n",
    "text:\n",
    "- \"\\nSay it like a poem\\n\"\n",
    "- model: \"replicate/ibm-granite/granite-3.1-8b-instruct\"\n",
    "- \"\\n\\nTranslate it to French\\n\"\n",
    "- model: \"replicate/ibm-granite/granite-3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b1959-e49b-48d3-b722-19ace9b981d2",
   "metadata": {},
   "source": [
    "## Chat templates\n",
    "\n",
    "The second call to the model in the above program submits the following prompt. PDL takes care of applying the appropriate chat templates and tags, and builds the background context implicitly. Chat templates make your program easier to port across models, since you do not need to specify control tokens by hand. All the user has to do is list the models they want to chain, PDL takes care of the rest.\n",
    "\n",
    "```\n",
    "<|start_of_role|>user<|end_of_role|>What is the meaning of life?\n",
    "<|end_of_text|>\n",
    "The meaning of life is a philosophical and metaphysical question related to the purpose or significance of life or existence in general. This concept has been approached by many perspectives including philosophy, religion, and science. Some people find meaning through personal growth, relationships, love, and through helping others. Others seek meaning through spirituality or religious beliefs. Ultimately, the meaning of life may be a personal and subjective experience.\n",
    "\n",
    "<|start_of_role|>user<|end_of_role|>Say it like a poem<|end_of_text|>\n",
    "Life's meaning, a question vast,\n",
    "In philosophy, religion, and science cast.\n",
    "Some find purpose in personal growth,\n",
    "In love and relationships, they find their troth.\n",
    "Others seek meaning through spirituality,\n",
    "In faith and belief, they find their reality.\n",
    "Ultimately, meaning is a personal quest,\n",
    "In life's journey, we are put to the test.\n",
    "\n",
    "<|start_of_role|>user<|end_of_role|>Translate it to French\n",
    "<|end_of_text|>\n",
    "<|start_of_role|>assistant<|end_of_role|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99681db4-8a43-4b06-92a6-8d140989f2ea",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "The following program shows a common prompting pattern: read some data, formulate a prompt using that data, submit to a model, and evaluate. In this program, we formulate a prompt for code explanation. The program first defines two variables: `code`, which holds the data we read, and `truth` for the ground truth. It then prints out the source code, formulates a prompts with the data, and calls a model to get an explanation. Finally, a Python code block uses the Levenshtein text distance metric and evaluate the explanation against the ground truth. This pipeline can similarly be applied to an entire data set to produce a jsonl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b2e25-72a4-4f70-ae83-40d77bed3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pdl\n",
    "defs:\n",
    "  code:\n",
    "    read: ./data.yaml\n",
    "    parser: yaml\n",
    "  truth:\n",
    "    read: ./ground_truth.txt\n",
    "text:\n",
    "- \"\\n${ code.source_code }\\n\"\n",
    "- model: \"replicate/ibm-granite/granite-3.1-8b-instruct\"\n",
    "  def: explanation\n",
    "  input: |\n",
    "      Here is some info about the location of the function in the repo.\n",
    "      repo: \n",
    "      ${ code.repo_info.repo }\n",
    "      path: ${ code.repo_info.path }\n",
    "      Function_name: ${ code.repo_info.function_name }\n",
    "\n",
    "\n",
    "      Explain the following code:\n",
    "      ```\n",
    "      ${ code.source_code }```\n",
    "- |\n",
    "\n",
    "  Evaluation:\n",
    "  The similarity (Levenshtein) between this answer and the ground truth is:\n",
    "- def: EVAL\n",
    "  lang: python\n",
    "  code: |\n",
    "    import textdistance\n",
    "    expl = \"\"\"\n",
    "    ${ explanation }\n",
    "    \"\"\"\n",
    "    truth = \"\"\"\n",
    "    ${ truth }\n",
    "    \"\"\"\n",
    "    result = textdistance.levenshtein.normalized_similarity(expl, truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0dd93-febb-408b-ae22-e829e02906e9",
   "metadata": {},
   "source": [
    "## Agentic Flow\n",
    "\n",
    "The following PDL program shows an agentic flow with a ReAct prompt pattern. It first reads some demonstrations to be used as few-shots. The ReAct pattern is captured with PDL control structures (repeat-until and if-then-else), and consists of cycling through thoughts, actions, and observations. The tools available are Wikipedia search, and calculator (as Python code). The agent decides when to search and when to calculate. The `spec` indicates a type for the output of the model when actions are produced, it is used to dynamically check outputs of models and fail when they don't conform to the expectation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef7096-b7a6-4966-8356-a306e701974b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%pdl --reset-context\n",
    "text:\n",
    "- read: demonstrations.txt\n",
    "  contribute: [context]\n",
    "- \"How many years ago was the discoverer of the Hudson River born? Keep in mind we are in 2024.\\n\"\n",
    "- repeat:\n",
    "    text:\n",
    "    - def: thought\n",
    "      model: replicate/ibm-granite/granite-3.1-8b-instruct\n",
    "      parameters:\n",
    "        stop: [\"Act:\"]\n",
    "        temperature: 0\n",
    "    - def: rawAction\n",
    "      model: replicate/ibm-granite/granite-3.1-8b-instruct\n",
    "      parameters:\n",
    "        stop: [\"\\n\"]\n",
    "        temperature: 0\n",
    "    - def: action\n",
    "      lang: python\n",
    "      parser: json\n",
    "      spec: {name: str, arguments: obj}\n",
    "      contribute: [context]\n",
    "      code:\n",
    "        |\n",
    "        result = '${ rawAction }'.replace(\"Act: \", \"\")\n",
    "    - def: observation\n",
    "      if: ${ action.name == \"Search\" }\n",
    "      then:\n",
    "        text:\n",
    "        - \"\\nObs: \"\n",
    "        - lang: python\n",
    "          code: |\n",
    "            import warnings, wikipedia\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            try:\n",
    "              result = wikipedia.summary(\"${ action.arguments.topic }\")\n",
    "            except wikipedia.WikipediaException as e:\n",
    "              result = str(e)\n",
    "        - \"\\n\"\n",
    "      else:\n",
    "        - if: ${ action.name == \"Calc\" }\n",
    "          then:\n",
    "            text:\n",
    "            - \"\\nObs: \"\n",
    "            - lang: python\n",
    "              code: result = ${ action.arguments.expr }\n",
    "            - \"\\n\"\n",
    "  until: ${ action.name != \"Search\" }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89438b62-29e4-472e-89ec-57c1626ffd44",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Since prompts are at the forefront, PDL makes users more productive in their trial-and-error with LLMs. Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7576532-aee3-4580-85fd-0b97bc503621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
