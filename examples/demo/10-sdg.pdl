defs:
  teacher_sys_prompt: You are a very knowledgeable AI Assistant that will faithfully assist the user with their task.
  teacher_model: ollama_chat/granite3.2:8b
  teacher_template:
    function:
      sys_prompt: str
      prompt: str
    return: <s> [INST] ${sys_prompt} ${prompt} [/INST]
  teacher_stop_token: </s>


  question_template_freeform:
    function:
      num_samples: int
      task_description: str
      icl_question: str
    spec: { introduction: str, principles: str, examples: str, generation: str, max_new_tokens: int }
    return:
      data:
        introduction: |
          You are asked to come up with a set of ${num_samples} diverse questions - ${task_description}.
        principles: |
          Please follow these guiding principles when generating responses:
          * Use proper grammar and punctuation.
          * Always generate safe and respectful content. Do not generate content that is harmful, abusive, or offensive.
          * Always generate content that is factually accurate and relevant to the prompt.
          * The questions should be clear and human-like.
          * The questions should be diverse and cover a wide range of topics.
          * The questions should not be template-based or generic, it should be very diverse.
          * Simply return the questions, do not return any answers or explanations.
          * Strictly adhere to the prompt and generate responses in the same style and format as the example.
          Use this format to generate the questions:
          ### Question 1:
        examples: |
          To better assist you with this task, here is an example:
          ### Question 1: ${icl_question}
        generation: |
          Now generate ${num_samples} such questions, remember to follow the principles mentioned above and use the same format as the examples. Remember to use the same style and format as the example above.
        max_new_tokens: 10000

  gen_questions_freeform_inner:
    function:
      num_samples: int
      task_description: str
      icl_question: str
      icl_answer: str
    spec: [{icl_question: str, icl_answer: str, question: str}]
    return:
      defs:
        prompt_data:
          call: ${question_template_freeform}
          spec: { introduction: str, principles: str, examples: str, generation: str, max_new_tokens: int }
          args:
            num_samples: ${num_samples}
            task_description: ${task_description}
            icl_question: ${icl_question}
        teacher_input:
          call: ${teacher_template}
          args:
            sys_prompt: ${teacher_sys_prompt}
            prompt: |-
              ${prompt_data.introduction}
              ${prompt_data.principles}
              ${prompt_data.examples}
              ${prompt_data.generation}
        teacher_output:
          model: ${teacher_model}
          input: ${teacher_input}
          parameters:
            temperature: 0
            stop: ["${teacher_stop_token}"]
            max_new_tokens: ${prompt_data.max_new_tokens}
          parser:
            regex: '### Question [0-9]+:\s*([^#\n]+)'
            mode: findall
      for:
        question: ${teacher_output}
      repeat:
        data:
          icl_question: ${icl_question}
          icl_answer: ${icl_answer}
          question: ${question}
      join:
        as: array

  gen_questions_freeform:
    function:
      task_description: str
      seed_examples: [{question: str, answer: str}]
    spec: [{icl_question: str, icl_answer: str, question: str}]
    return:
      defs:
        list_of_lists:
          for:
            example: ${seed_examples}
          repeat:
            call: ${gen_questions_freeform_inner}
            args:
              num_samples: 2
              task_description: ${task_description}
              icl_question: ${example.question}
              icl_answer: ${example.answer}
          join:
            as: array
      lang: python
      code: | # flatten list_of_lists into simple list
          result = [q for qs in ${list_of_lists} for q in qs]


  filter_questions_template:
    function:
      task_description: str
      question: str
    spec: {introduction: str, principles: str, generation: str, max_new_tokens: int}
    return:
      data:
        introduction: |
          Please act as an impartial judge and evaluate the questions generated by an AI assistant displayed below. Evaluate whether or not the question is a good question of how AI Assistant should respond to the user's instruction. Please assign a score using a binary 0/1 scale.
        principles: |
          Here are the requirements:
          * A large language model should be able to complete the question. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.
          * The questions should be in English.
          * The questions should be 1 to 2 sentences long and should be properly formatted.
          * The question should not be offensive, abusive, or harmful. It should be safe and respectful.
          * The question should be relevant to the task given - ${task_description}.
          If the question meets the above requirements, please rate it 1. If not, please rate it 0.
        generation: |
          Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the question on a scale of 0 or 1 as mentioned above by strictly following this format: \"[[rating]]\", for example: \"Rating: [[1]]\"
          Here is the question you need to evaluate:
          ${question}
        max_new_tokens: 256

  # https://github.com/instruct-lab/datagen-pipeline/blob/main/sdg/filter_questions.py
  filter_questions_inner:
    function:
      task_description: str
      question: str
    spec: float
    return:
      defs:
        prompt_data:
          call: ${filter_questions_template}
          spec: {introduction: str, principles: str, generation: str, max_new_tokens: int}
          args:
            task_description: ${task_description}
            question: ${question}
        teacher_input:
          call: ${teacher_template}
          args:
            sys_prompt: ${teacher_sys_prompt}
            prompt: |-
              ${prompt_data.introduction}
              ${prompt_data.principles}
              ${prompt_data.generation}
        teacher_output:
          model: ${teacher_model}
          input: ${teacher_input}
          parameters:
            stop: ["${teacher_stop_token}"]
            max_new_tokens: ${prompt_data.max_new_tokens}
            temperature: 0
          parser:
            spec: { "rating": str }
            # regex: "Rating.*\\[\\[(?P<rating>\\d+\\.?\\d*)\\]\\]"
            regex: 'Rating.*\[\[(?P<rating>\d+\.?\d*)\]\]'
            mode: search
      data: ${ teacher_output.rating | float }

  filter_questions:
    function:
      task_description: str
      questions: [{icl_question: str, icl_answer: str, question: str}]
    spec: [{icl_question: str, icl_answer: str, question: str}]
    return:
      defs:
        list_of_pairs:
          for:
            question: ${questions}
          repeat:
            defs:
              filter_output:
                call: ${filter_questions_inner}
                args:
                  task_description: ${task_description}
                  question: ${question.question}
            data:
              question: ${question}
              keep: ${filter_output}
          join:
            as: array
        filtered:
          lang: python
          code: | # keep only if "keep" column is non-zero
              result = [p["question"] for p in ${ list_of_pairs } if p["keep"]]
      data: ${filtered}


  answer_template:
    function:
      icl_question: str
      icl_response: str
      question: str
    spec: {introduction: str, principles: str, examples: str, generation: str, max_new_tokens: int, additional_stop_tokens: [str]}
    return:
      data:
        introduction: Your task is to faithfully follow the user's prompt and generate a response.
        principles: |
          Please follow these guiding principles when generating responses:
          * Use proper grammar and punctuation.
          * Always generate safe and respectful content. Do not generate content that is harmful, abusive, or offensive.
          * Always generate content that is factually accurate and relevant to the prompt.
          * Strictly adhere to the prompt and generate responses in the same style and format as the example.
        examples: |
          To better assist you with this task, here is an example:
          [Question]
          ${icl_question}
          [Response]
          ${icl_response}
        generation: |
          Now generate a response to the following prompt. Remember to use the same style and format as the example above.
          [Question]
          ${question}
          [Response]
        max_new_tokens: 2048
        additional_stop_tokens:
        - "[Question]"


  gen_answers_inner:
    function:
      question: {icl_question: str, icl_answer: str, question: str}
    spec: {question: str, answer: str}
    return:
      defs:
        prompt_data:
          call: ${answer_template}
          spec: {introduction: str, principles: str, examples: str, generation: str, max_new_tokens: int, additional_stop_tokens: [str]}
          args:
            icl_question: ${question.icl_question}
            icl_response: ${question.icl_answer}
            question: ${question.question}
        teacher_input:
          call: ${teacher_template}
          args:
            sys_prompt: ${teacher_sys_prompt}
            prompt: |-
              ${prompt_data.introduction}
              ${prompt_data.principles}
              ${prompt_data.examples}
              ${prompt_data.generation}
        teacher_output:
          model: ${teacher_model}
          input: ${teacher_input}
          parameters:
            stop: ["${ ([teacher_stop_token] + prompt_data.additional_stop_tokens) | join(',') }"]
            max_new_tokens: ${prompt_data.max_new_tokens}
            temperature: 0
        parsed_answer:
          lang: python
          code: | # parse model output
            result = """ ${teacher_output} """.strip()
            if "[Response]" in result:
              result = result[result.find("[Response]") + len("[Response]"):]
            if "[Question]" in result:
              result = result[:result.find("[Question]")]
      data:
        question: ${question.question}
        answer: ${parsed_answer}

  gen_answers:
    function:
      questions: [{icl_question: str, icl_answer: str, question: str}]
    spec: [{question: str, answer: str}]
    return:
      defs:
        all_results:
          spec: [{question: str, answer: str}]
          for:
            question: ${ questions }
          repeat:
            call: ${gen_answers_inner}
            args:
              question: ${question}
          join:
            as: array
      lang: python
      spec: [{question: str, answer: str}]
      code: | # keep only if answer non-empty
          result = [r for r in ${all_results} if len(r["answer"]) > 0]


  filter_qa_template:
    function:
      question: str
      answer: str
    spec: {introduction: str, principles: str, generation: str, max_new_tokens: int}
    return:
      data:
        introduction: |
          Please act as an impartial judge and evaluate the quality of the answer provided by an AI assistant to the questions displayed below. Evaluate whether or not the answer is a good example of how AI Assistant should respond to the user's instruction. Please assign a score using the following 3-point scale.
        principles: |
          1: It means the answer is incorrect, irrelevant, unsafe or provides incomplete and garbage information. For instance, the answer may be factually wrong, off-topic, or filled with irrelevant content that doesn't address the user's question or it could be incomplete and hanging. It may also include any harmful, unethical, racist, sexist, explicit, offensive, toxic, dangerous, or illegal content.

          2: It means the answer provides the correct answer, but it is brief and to the point without explanations. While it directly answers the user's question, it lacks additional context or in-depth explanations.

          3: It means the answer is a perfect answer from an AI Assistant. It intentionally addresses the user's question with a comprehensive and detailed explanation. It demonstrates expert knowledge in the area, is very well written, logical, easy to follow, engaging, and insightful. And the answer is safe and does not include any harmful content.
        generation: |
          Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the answer on a scale of 1 to 3 as mentioned above by strictly following this format: \"[[rating]]\", for example: \"Rating: [[1]]\"

          Here is the question and the answer you need to evaluate:
          [Question]
          ${question}
          [Answer]
          ${answer}
        max_new_tokens: 256

  filter_question_answer_pair_inner:
    function:
      question: str
      answer: str
    spec: float
    return:
      defs:
        prompt_data:
          call: ${filter_qa_template}
          spec: {introduction: str, principles: str, generation: str, max_new_tokens: int}
          args:
            question: ${question}
            answer: ${answer}
        teacher_input:
          call: ${teacher_template}
          args:
            sys_prompt: ${teacher_sys_prompt}
            prompt: |-
              ${prompt_data.introduction}
              ${prompt_data.principles}
              ${prompt_data.generation}
        teacher_output:
          model: ${teacher_model}
          input: ${teacher_input}
          parameters:
            stop: ["${teacher_stop_token}"]
            max_new_tokens: ${prompt_data.max_new_tokens}
            temperature: 0
          parser:
            spec: { "rating": str }
            regex: 'Rating.*\[\[(?P<rating>\d+\.?\d*)\]\]'
            mode: search
      data: ${ (teacher_output.rating if teacher_output.rating is not none else 0.0) | float}

  filter_question_answer_pair:
    function:
      qa_pairs: [{question: str, answer: str}]
    spec: [{question: str, answer: str}]
    return:
      defs:
        ratings:
          for:
            qa_pair: ${qa_pairs}
          repeat:
            defs:
              filter_output:
                call: ${filter_question_answer_pair_inner}
                spec: float
                args:
                  question: ${qa_pair.question}
                  answer: ${qa_pair.answer}
            data:
              qa_pair: ${qa_pair}
              rating: ${filter_output}
          join:
            as: array
        filtered:
          lang: python
          spec: [{question: str, answer: str}]
          code: | # keep only if rating is at least two
              result = [p["qa_pair"] for p in ${ratings} if p["rating"] >= 2]
      data: ${filtered}


text:
- "----- Loading seed examples -----\n\n"
- def: seed_examples
  read: ./qna.yaml
  parser: yaml
- "\n\n----- Generating questions -----\n\n"
- def: generated_questions
  call: ${gen_questions_freeform}
  spec: [{icl_question: str, icl_answer: str, question: str}]
  args:
    task_description: ${seed_examples.task_description}
    seed_examples: ${seed_examples.seed_examples}
- "\n\n----- Filtering questions -----\n\n"
- def: filtered_questions
  call: ${filter_questions}
  spec: [{icl_question: str, icl_answer: str, question: str}]
  args:
    task_description: ${seed_examples.task_description}
    questions: ${generated_questions}
- "\n\n----- Generating answers -----\n\n"
- def: qa_pairs
  call: ${gen_answers}
  args:
    questions: ${filtered_questions}
- "\n\n----- Filtering QA pairs -----\n\n"
- call: ${filter_question_answer_pair}
  args:
    qa_pairs: ${qa_pairs}

