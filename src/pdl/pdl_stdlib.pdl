
defs: 
  reward:
    function:
      response: 
      evaluation: string
    return:
      defs:
        contents: ${ response['choices'][0].logprobs.content}
      lastOf:
      - for: 
          content: ${ contents }
        repeat: 
          if: ${ content.token == evaluation }
          then: 
            def: top_logprobs
            data: ${ content.top_logprobs }
      - for: 
          tp: ${ top_logprobs }
        repeat: 
          match: ${ tp.token | lower }
          with:
          - case: "true"
            then:
              data: ${ tp.logprob }
              def: lp_y
          - case: "false"
            then: 
              data: ${ tp.logprob }
              def: lp_n
      - lang: python  
        code: |
          import math 
          try:
            lp_y
          except NameError:
            lp_y = -10
          try:
            lp_n
          except NameError:
            lp_n = -10
          result = math.log(math.exp(lp_y) / (math.exp(lp_y) + math.exp(lp_n)))

  requirements:
    object:
      evaluation:
        function:
          requirement: string
          response: string
          llm_as_judge: {optional: string}
        return:
          lastOf:
          - #model: ${ llm_as_judge | default('watsonx/meta-llama/llama-3-3-70b-instruct') }
            model: ${ llm_as_judge | default('watsonx/openai/gpt-oss-120b') }
            def: evaluation
            input: |
              Problem: ${ requirement }
              Solution: ${ response }
              
              Is the solution correct? Respond with only ('true'/'false')
            modelResponse: out
            parameters:
              logprobs: true
              top_logprobs: 5
              response_format: {'type': 'json_schema', 'json_schema': {'name': 'schema', 'schema': {'enum': [True, False]}, 'strict': True}}
          - ${ reward(response=out, evaluation=evaluation) }
          
          
      transformContext:
        function:
          requirement: string
          response: string
          model: {optional: string}
        return: 
          lastOf:
          - model: ${ model | default('ollama_chat/granite3.3:8b') }
            input: |
              The following requirement is not satisfied, what instruction can be added to get the correct answer?
              Requirement: ${ requirement }
              Answer with only the instruction.
          - ${ pdl_context }



  

