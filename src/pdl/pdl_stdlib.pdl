
defs: 
  reward:
    function:
      response: 
      evaluation: string
    return:
      lang: python  
      code: |
        import math

        if (evaluation != 'true' and evaluation != 'false'):
          raise Exception("Wrong value")

        contents =  response['choices'][0]['logprobs']['content']
        top_logprobs = []

        contents_keys = {c["token"] for c in contents}
        if evaluation in contents_keys:
          exact_match = True
        else:
          exact_match = False

        def match(v, c, exact):
          if exact:
            return v == c
          return v.startswith(c)

        for content in contents:
          if match(evaluation, content['token'], exact_match):
            top_logprobs = content['top_logprobs']
            lp_evaluation = content['logprob']
            break
        
        if evaluation == 'true':
          lp_y = lp_evaluation
          other = 'false'
        else: 
          lp_n = lp_evaluation
          other = 'true'

        if other in contents_keys:
          exact_match = True
        else:
          exact_match = False

        lp_other = math.inf
        for tp in top_logprobs:
          if match(other, tp['token'], exact_match):
            lp_other = tp['logprob']
            break
          lp_other = min(lp_other, tp['logprob'])

        # if math.isinf(lp_other):
        #   print("XXXXXXXXXXXXXXXXXXXXXXXXXXXX")
        #   print("XXXX lp_other is not defined")
        #   print(f"{evaluation=}")
        #   print(f"{other=}")
        #   print(f"{exact_match=}")
        #   print(f"top_logprobs tokens:")
        #   for tp in top_logprobs:
        #     print(f"    {tp['token']}")
        #   print("XXXXXXXXXXXXXXXXXXXXXXXXXXXX")

        if other == 'true':
          lp_y = lp_other
        else: 
          lp_n = lp_other

        if math.exp(lp_y) == 0.0:
          result = -math.inf
        else:
          result = math.log(math.exp(lp_y) / (math.exp(lp_y) + math.exp(lp_n)))

        # print(f"evaluation: { evaluation }")
        # print(f"exact_match: { exact_match }")
        # print(f"lp_y: { lp_y }")
        # print(f"lp_n: { lp_n }")
        # print(f"reward: { result }")
        # print(contents_keys)


  llm_as_judge: 
    function:
      model: string
      prompt: string
    return:
      lastOf:
      - model: ${ model }
        def: evaluation
        input: |
          ${ prompt }
        modelResponse: out
        parameters:
          temperature: 0
          logprobs: true
          top_logprobs: 5
          response_format: {'type': 'json_schema', 'json_schema': {'name': 'schema', 'schema': {'enum': [True, False]}, 'strict': True}}
      - def: score
        data: ${ reward(response=out, evaluation=evaluation) }
      retry: 3
      fallback:
        lang: python
        code: |
          raise Exception(f"Llm-as-judge failed!\n judge prompt: {prompt}")

  get_judge_prompt:
    function:
      model: string
      expectation: string
      response: string
    return:
      match: ${ model }
      with: 
      - case: watsonx/ibm/granite-4-h-small
        then: | 
          Your task is to determine if the following statement is true:
                  
          ${ expectation }
          
          Here is the solution: ${ response }

          Is the statement correct? Respond with only ('true'/'false')

      - case: watsonx/openai/gpt-oss-120b
        then: | 
          Your task is to determine if the following statement is true:
                  
          ${ expectation }
          
          Here is the solution: ${ response }

          Is the statement correct? Respond with only ('true'/'false')
      - then: | 
          Your task is to determine if the following statement is true:
                  
          ${ expectation }
          
          Here is the solution: ${ response }

          Is the statement correct? Respond with only ('true'/'false')



  expectations:
    object:
      feedback:
        function:
          expectation: string
          response: 
          pdl_llm_as_judge: string
          pdl_llm_context_transformer: string
        return:
          lastOf:
          - def: score
            call: ${ llm_as_judge }
            args:
              model: ${ pdl_llm_as_judge }
              prompt: ${ get_judge_prompt(pdl_llm_as_judge, expectation, response) }
          - if: ${ score < -0.69 }
            then:
              lastOf:
              - model: ${ pdl_llm_context_transformer }
                def: instruction
                input: |
                  The following requirement is not satisfied, what instruction can be added to get the correct answer?
                  requirement: ${ expectation }
                  Answer with only the instruction.
              - array:
                - ${ score }
                - ${ instruction }
            else:
              ${ score }
          
          





